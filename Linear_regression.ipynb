{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sverdoot/optimizer-SUG-torch/blob/master/Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBzga5dzZuI",
        "colab_type": "code",
        "outputId": "a25cd7b9-2c95-41f1-b570-ba44817b16b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJzr58v_puOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "import torch.autograd\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEcxhMj354nl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Optimization project\")\n",
        "from sug import *\n",
        "\n",
        "os.getcwd()\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Optimization project/LINREG\"\n",
        "#directory = os.path.dirname(file_path)\n",
        "\n",
        "try:\n",
        "    os.stat(file_path)\n",
        "except:\n",
        "    os.mkdir(file_path)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8JzUJlfWKgh",
        "colab_type": "code",
        "outputId": "2429304c-c5a6-4464-8f72-0ef57f508c9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pA8A5BaJ-1-A",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class SUG(Optimizer):\n",
        "    def __init__(self, params, l_0, d_0=0, prob=1., eps=1e-4, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if l_0 < 0.0:\n",
        "            raise ValueError(\"Invalid Lipsitz constant of gradient: {}\".format(l_0))\n",
        "        if d_0 < 0.0:\n",
        "            raise ValueError(\"Invalid disperion of gradient: {}\".format(d_0))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(L=l_0, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        self.Lips = l_0\n",
        "        self.prev_Lips = l_0\n",
        "        self.D_0 = d_0\n",
        "        self.eps = eps\n",
        "        self.prob = prob\n",
        "        self.start_param = params\n",
        "        self.upd_sq_grad_norm = None\n",
        "        self.sq_grad_norm = None\n",
        "        self.loss = torch.tensor(0.)\n",
        "        self.cur_loss = 0\n",
        "        self.closure = None\n",
        "        super(SUG, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SUG, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def comp_batch_size(self):\n",
        "        \"\"\"Returns optimal batch size for given d_0, eps and l_0;\n",
        "\n",
        "        \"\"\"\n",
        "        return math.ceil(2 * self.D_0 * self.eps / self.prev_Lips)\n",
        "\n",
        "    def step(self, loss, closure):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            loss : current loss\n",
        "\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        self.start_params = []\n",
        "        self.loss = loss\n",
        "        self.sq_grad_norm = 0\n",
        "        self.cur_loss = loss\n",
        "        self.closure = closure\n",
        "        for gr_idx, group in enumerate(self.param_groups):\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            self.start_params.append([])\n",
        "            for p_idx, p in enumerate(group['params']):\n",
        "                self.start_params[gr_idx].append([p.data.clone()])\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                self.start_params[gr_idx][p_idx].append(p.grad.data.clone())\n",
        "                d_p = self.start_params[gr_idx][p_idx][1]\n",
        "                p_ = self.start_params[gr_idx][p_idx][0]\n",
        "                \n",
        "                \n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                    self.cur_loss += weight_decay * torch.sum(p * p).item()\n",
        "                   \n",
        "                \n",
        "                self.sq_grad_norm += torch.sum(d_p * d_p).item()\n",
        "                \n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.zeros_like(p.data)\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "                self.start_params[gr_idx][p_idx][1] = d_p\n",
        "                \n",
        "        i = 0\n",
        "        self.Lips = max(self.prev_Lips / 2, 0.1)\n",
        "        difference = -1\n",
        "        while difference < 0 or i == 0:\n",
        "            if (i > 0): \n",
        "                self.Lips = max(self.Lips * 2, 0.1)\n",
        "            for gr_idx, group in enumerate(self.param_groups):\n",
        "                for p_idx, p in enumerate(group['params']):\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    start_param_val = self.start_params[gr_idx][p_idx][0]\n",
        "                    start_param_grad = self.start_params[gr_idx][p_idx][1]\n",
        "                    p.data = start_param_val - 1/(2*self.Lips) * start_param_grad\n",
        "            difference, upd_loss = self.stop_criteria()\n",
        "            i += 1\n",
        "        self.prev_Lips = self.Lips\n",
        "\n",
        "        return self.Lips, i\n",
        "\n",
        "    def stop_criteria(self):\n",
        "        \"\"\"Checks if the Lipsitz constant of gradient is appropriate\n",
        "        \n",
        "           <g(x_k), w_k - x_k> + 2L_k / 2 ||x_k - w_k||^2 = - 1 / (2L_k)||g(x_k)||^2 + 1 / (4L_k)||g(x_k)||^2 = -1 / (4L_k)||g(x_k)||^2                \n",
        "        \"\"\"\n",
        "        upd_loss = self.closure()\n",
        "        major =  self.cur_loss - 1 / (4 * self.Lips) * self.sq_grad_norm\n",
        "        return major - upd_loss - self.l2_reg() + self.eps / 10, upd_loss\n",
        "\n",
        "    def get_lipsitz_const(self):\n",
        "        \"\"\"Returns current Lipsitz constant of the gradient of the loss function\n",
        "        \"\"\"\n",
        "        return self.Lips\n",
        "    \n",
        "    def get_sq_grad(self):\n",
        "        \"\"\"Returns the current second norm of the gradient of the loss function \n",
        "           calculated by the formula\n",
        "           \n",
        "           ||f'(p_1,...,p_n)||_2^2 ~ \\sum\\limits_{i=1}^n ((df/dp_i) * (df/dp_i))(p1,...,p_n))\n",
        "           \n",
        "        \"\"\"\n",
        "        self.upd_sq_grad_norm = 0\n",
        "        for gr_idx, group in enumerate(self.param_groups):\n",
        "            for p_idx, p in enumerate(group['params']):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                self.upd_sq_grad_norm += torch.sum(p.grad.data * p.grad.data).item()\n",
        "        \n",
        "        return self.upd_sq_grad_norm\n",
        "    \n",
        "    def l2_reg(self):\n",
        "        \"\"\"Returns the current l2 regularization addiction\n",
        "           \n",
        "        \"\"\"\n",
        "        self.upd_l2_reg = 0\n",
        "        for gr_idx, group in enumerate(self.param_groups):\n",
        "            weight_decay = group['weight_decay']\n",
        "            if weight_decay != 0:\n",
        "                for p_idx, p in enumerate(group['params']):\n",
        "                    self.upd_l2_reg += weight_decay * torch.sum(p * p).item()\n",
        "        \n",
        "        return self.upd_l2_reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8yqvjmJqKVH",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vBOn87D8h10",
        "colab_type": "text"
      },
      "source": [
        "$x_i \\sim \\mathcal{N}(0, I),~~i=1..n,~~I\\in \\mathbb{R^{m^2}}$\n",
        "\n",
        "$y_i = \\theta^Tx_i + \\epsilon,~~\\epsilon \\sim N(0,\\sigma^2)$\n",
        "\n",
        "$y = X \\theta +\\epsilon,~~\\epsilon \\sim \\mathcal N(0,\\Sigma)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfEaNeh5qJLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile data\n",
        "torch.manual_seed(999)\n",
        "n_rows = 1000000\n",
        "n_columns = 100\n",
        "X = torch.randn(n_rows, n_columns) \n",
        "true_weights = torch.ones(n_columns) * 5\n",
        "y = X @ true_weights + torch.randn(n_rows) * 0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71tOoNMghCD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data\n",
        "val_size = math.ceil(0.15 * X.size(0))\n",
        "test_size = math.ceil(0.3 * X.size(0))\n",
        "train_size = X.size(0) - val_size - test_size\n",
        "\n",
        "X_train, y_train = X[: train_size], y[: train_size]\n",
        "X_test, y_test = X[train_size : train_size + test_size], y[train_size : train_size + test_size]\n",
        "X_val, y_val = X[- val_size :], y[- val_size :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9E_tYQV6V8l",
        "colab_type": "text"
      },
      "source": [
        "## Loss function\n",
        "\n",
        "$$L(\\theta, X) = \\dfrac{1}{m}\\sum\\limits_{i=1}^m | x_i\\theta -  y_i|^2 = \\dfrac{1}{m}\\|X\\theta- y\\|_2^2$$\n",
        "\n",
        "$$\\nabla_{\\theta}L(\\theta,X) = \\dfrac{2}{m}\\sum\\limits_{i=1}^m x_i^T (x_i \\theta-y_i) $$\n",
        "\n",
        "The Lipsitz constant of the gradient may be determined straight from the definition :\n",
        "\n",
        "$$\\|\\nabla_{\\theta_1}L(\\theta_1,X)-\\nabla_{\\theta_2}L(\\theta_2,X)\\| _2 \\le L\\|\\theta_1 - \\theta_2\\|_2, ~~~\\forall \\theta_1,\\theta_2 \\in \\mathbb{R^m}$$\n",
        "\n",
        "$$\\|\\nabla_{\\theta_1}L(\\theta_1,X)-\\nabla_{\\theta_2}L(\\theta_2,X)\\| _2 = \n",
        "\\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T (x_i (\\theta_1-\\theta_2))\\|_2 \\le \\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2 \\|\\theta_1-\\theta_2\\|_2$$\n",
        "\n",
        "$$\\Rightarrow  L \\le \\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2$$\n",
        " \n",
        "$$As ~\\sum\\limits_{i=1}^m x_i^T x_i~~ is~ simmetric, $$\n",
        " \n",
        "$$\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2 = \\lambda_{max}\\left(\\sum\\limits_{i=1}^mx_i^T x_i\\right)$$\n",
        " \n",
        "Also it is possible to find L from the following statement:\n",
        "\n",
        "$$\\|\\nabla_\\theta^2 L(\\theta, X)\\|_2^2  = \\lambda_{max}\\left(\\nabla_\\theta^2 L(\\theta, X)\\right) \\le L,~~~\\forall \\theta \\in \\mathbb{R^m}$$ \n",
        "\n",
        "$$\\nabla_{\\theta}^2 L(\\theta,X) = \\dfrac{2}{m}\\sum\\limits_{i=1}^mx_i^T x_i$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWHkvbKa5beA",
        "colab_type": "text"
      },
      "source": [
        "Maximum eigen value of the matrix $A$ can be found with the following itrative method:\n",
        "\n",
        "$$y_{i+1} = A x_i$$\n",
        "$$x_{i+1} = y_{i+1} / \\|y_{i+1}\\|_2$$\n",
        "$$i = i+1$$\n",
        " \n",
        " \n",
        "As $x_i$ is a normalized real vector when $i>0$,    $~~~\\lambda_i = x_i^*Ax_i = x_i^T A x_i$. \n",
        "\n",
        "The one should repeat the iteration until convergence of $\\lambda$.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIY_AAMrqZw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss(reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAIMHdEm_0Hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# estimate the Lipsitz constant of the gradient\n",
        "\n",
        "def mse_hessian(X):\n",
        "    hess = 0\n",
        "    for x in X:\n",
        "        hess += 2 / X.size(0) * torch.ger(x, x)\n",
        "    return hess    \n",
        "  \n",
        "hess = mse_hessian(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mng53v520J2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_eigval(A, x=None, eps=1e-7):\n",
        "    if x is None:\n",
        "        x = torch.randn(A.size(1))\n",
        "    lam = 1\n",
        "    dif = 1\n",
        "    while (dif > eps):\n",
        "        lam_prev = lam\n",
        "        y = A @ x\n",
        "        x = y / torch.norm(y, 2)\n",
        "        lam = (x @ A @ x).item()\n",
        "        dif = np.abs(lam - lam_prev)\n",
        "    return lam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWH0g9nL1hK7",
        "colab_type": "code",
        "outputId": "6377572a-56da-4f54-f7ea-72d96475ed92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "L = max_eigval(hess)\n",
        "print(\"the Lipsitz constant of the gradient does not exceed {}\".format(- math.floor(- L * 10 ** 3) / 10 ** 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the Lipsitz constant of the gradient does not exceed 2.053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2CrOMinf1Zz",
        "colab_type": "text"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANbit2lVI-dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, n_columns):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(n_columns))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x @ self.weights\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Cqx_BIf91i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, X_train, y_train, n_epochs=1, batch_size=4, print_every=1, X_val=None, y_val=None):\n",
        "    model.to(device)\n",
        "    tr_losses, val_losses, lips, grad = ([] for i in range(4))\n",
        "    batch_per_ep = X_train.size(0) // batch_size\n",
        "    for ep in range(n_epochs):\n",
        "        model.train()\n",
        "        for i in range(batch_per_ep):\n",
        "            inputs, y = Variable(X_train[i*batch_size:(i+1)*batch_size]).to(device), Variable(y_train[i*batch_size:(i+1)*batch_size]).to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, y)\n",
        "            tr_losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            if optimizer.__class__.__name__ != 'SUG':\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                def closure():\n",
        "                    optimizer.zero_grad()\n",
        "                    upd_outputs = model(inputs)\n",
        "                    upd_loss = criterion(upd_outputs, y)\n",
        "                    return upd_loss.item()\n",
        "                _, _ = optimizer.step(loss.item(), closure)\n",
        "                lips.append(optimizer.get_lipsitz_const())\n",
        "                grad.append(optimizer.get_sq_grad)\n",
        "                \n",
        "            model.zero_grad()\n",
        "            model.eval()\n",
        "            X, y = Variable(X_val).to(device), Variable(y_val).to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y) \n",
        "            val_losses.append(loss.item())\n",
        "            model.train()\n",
        "    if ep % print_every == 0:\n",
        "        print(\"Epoch: {}, training loss: {}, validation loss: {}\".format(ep, sum(tr_losses[-batch_per_ep:])/batch_per_ep, val_losses[-1]))\n",
        "    \n",
        "    return tr_losses, val_losses, lips, grad            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cutdudpZ2IHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrs = [0.5, 0.1, 0.01]\n",
        "n_epochs = 1\n",
        "tr_loss = {}\n",
        "tr_loss['sgd'] = {}\n",
        "val_loss = {}\n",
        "val_loss['sgd'] = {}\n",
        "criterion = nn.MSELoss(reduction=\"mean\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kfYn4BRFofG",
        "colab_type": "code",
        "outputId": "12166596-fa40-478b-eff0-f3ffa9ca501d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for lr in lrs:\n",
        "    print(\"SGD lr={} momentum 0. weight_decay 1e-3\".format(lr))\n",
        "    model = LinearRegressionModel(n_columns)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0., weight_decay=1e-3)\n",
        "    tr_loss['sgd'][lr], val_loss['sgd'][lr], lips, grad = train(model, optimizer, X_train, y_train, n_epochs=n_epochs, batch_size=512, X_val=X_val, y_val=y_val, print_every=1)\n",
        "    print(\"Test score: {:.4}\\n\\n\".format(criterion(model(X_test.to(device)), y_test.to(device)).item()))\n",
        "    states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sgd'][lr],\n",
        "            'val_loss' : val_loss['sgd'][lr],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad\n",
        "        }\n",
        "    torch.save(states, './LINREG/lr_'+str(lr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD lr=0.5 momentum 0. weight_decay 1e-3\n",
            "Epoch: 0, training loss: 3.3324894935059146, validation loss: 0.663571298122406\n",
            "Test score: 0.6624\n",
            "\n",
            "\n",
            "SGD lr=0.1 momentum 0. weight_decay 1e-3\n",
            "Epoch: 0, training loss: 7.515337376121702, validation loss: 0.5041121244430542\n",
            "Test score: 0.5042\n",
            "\n",
            "\n",
            "SGD lr=0.01 momentum 0. weight_decay 1e-3\n",
            "Epoch: 0, training loss: 60.27786558996921, validation loss: 0.4917539060115814\n",
            "Test score: 0.4924\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfrfJb2T9OpB",
        "colab_type": "code",
        "outputId": "cafb1efb-0df7-449b-f260-daee001f4d9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"SUG l_0=2.1 weight_decay=1e-3\")\n",
        "model = LinearRegressionModel(n_columns)\n",
        "sgd = SUG(model.parameters(), l_0=2.1, weight_decay=1e-3)\n",
        "tr_loss['sug'], val_loss['sug'], lips, grad = train(model, sgd, X_train, y_train, n_epochs=1, batch_size=512, X_val=X_val, y_val=y_val, print_every=1)\n",
        "#print(\"\\nTrained weights: \\n{}\\n\".format(list(model.parameters())[0].cpu().detach().numpy()))\n",
        "print(\"Test score: {:.4}\".format(criterion(model(X_test.to(device)), y_test.to(device)).item()))\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sug'],\n",
        "            'val_loss' : val_loss['sug'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad\n",
        "        }\n",
        "torch.save(states, './LINREG/lr_sug')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUG l_0=2.1 weight_decay=1e-3\n",
            "Epoch: 0, training loss: 3.625562452681682, validation loss: 0.5332871675491333\n",
            "Test score: 0.5335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuP0ZNPGnjKn",
        "colab_type": "code",
        "outputId": "ed450c0d-078d-4c5d-a802-d02a224f1a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Trained weights:\\n{}\\n\".format(list(lr_model.coef_)))\n",
        "\n",
        "print(\"{:.4}\".format(criterion(torch.tensor(lr_model.predict(X_test)), y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trained weights:\n",
            "[5.000724, 4.9999266, 5.0013065, 4.998826, 4.999601, 5.0004044, 4.999807, 5.0003324, 4.9998713, 5.000539, 5.000581, 4.999488, 5.0012116, 4.999505, 5.0008817, 5.001804, 5.000177, 4.9986477, 5.000984, 4.9998875, 4.9995165, 5.000615, 4.9999037, 5.0002794, 4.99903, 4.999418, 4.9999304, 5.00036, 4.999962, 5.0000887, 5.000258, 5.0010433, 4.998993, 4.998514, 5.001095, 5.0027943, 4.9993105, 4.9999247, 4.9977074, 4.9997997, 4.999658, 5.001179, 4.9991293, 5.0015984, 5.0008783, 5.000133, 5.0004673, 5.000086, 4.9992847, 4.9987707, 5.000008, 5.001558, 4.9988203, 5.000719, 4.999448, 5.001664, 5.0009217, 4.999479, 4.999565, 4.999815, 4.998951, 5.000894, 5.0004435, 5.0008864, 4.998207, 5.0011196, 5.0005913, 4.999487, 5.0000505, 5.000177, 5.000865, 4.998932, 4.997824, 4.99936, 4.998658, 5.000172, 5.0004015, 4.9999013, 4.9979987, 5.001012, 5.00029, 4.999296, 5.00208, 4.99966, 5.0009828, 4.9990106, 4.9997272, 4.998647, 4.9997196, 5.0009317, 5.001313, 4.999817, 4.9982986, 5.0004134, 4.9991245, 4.9986596, 5.0009613, 4.9985237, 5.001109, 4.999803]\n",
            "\n",
            "0.4908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTlHfbRIFT8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}