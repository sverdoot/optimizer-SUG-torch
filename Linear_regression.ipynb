{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sverdoot/optimizer-SUG-torch/blob/master/Linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aSBzga5dzZuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33fd6370-8ff4-4393-e3bb-763bf69e454e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MJzr58v_puOA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "import torch.autograd\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEcxhMj354nl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Optimization project\")\n",
        "from sug import *\n",
        "\n",
        "os.getcwd()\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/Optimization project/LINREG\"\n",
        "#directory = os.path.dirname(file_path)\n",
        "\n",
        "try:\n",
        "    os.stat(file_path)\n",
        "except:\n",
        "    os.mkdir(file_path)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v8JzUJlfWKgh",
        "colab_type": "code",
        "outputId": "e3d6260a-94a5-4890-9df3-87a78e29baf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "O8yqvjmJqKVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "9vBOn87D8h10",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$x_i \\sim \\mathcal{N}(0, I),~~i=1..n,~~I\\in \\mathbb{R^{m^2}}$\n",
        "\n",
        "$y_i = \\theta^Tx_i + \\epsilon,~~\\epsilon \\sim N(0,\\sigma^2)$\n",
        "\n",
        "$y = X \\theta +\\epsilon,~~\\epsilon \\sim \\mathcal N(0,\\Sigma)$"
      ]
    },
    {
      "metadata": {
        "id": "nfEaNeh5qJLb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compile data\n",
        "n_rows = 1000000\n",
        "n_columns = 100\n",
        "X = torch.randn(n_rows, n_columns) \n",
        "true_weights = torch.ones(n_columns) * 5\n",
        "y = X @ true_weights + torch.randn(n_rows) * 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "71tOoNMghCD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data\n",
        "val_size = math.ceil(0.15 * X.size(0))\n",
        "test_size = math.ceil(0.3 * X.size(0))\n",
        "train_size = X.size(0) - val_size - test_size\n",
        "\n",
        "X_train, y_train = X[: train_size], y[: train_size]\n",
        "X_test, y_test = X[train_size : train_size + test_size], y[train_size : train_size + test_size]\n",
        "X_val, y_val = X[- val_size :], y[- val_size :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9E_tYQV6V8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss function\n",
        "\n",
        "$$L(\\theta, X) = \\dfrac{1}{m}\\sum\\limits_{i=1}^m | x_i\\theta -  y_i|^2 = \\dfrac{1}{m}\\|X\\theta- y\\|_2^2$$\n",
        "\n",
        "$$\\nabla_{\\theta}L(\\theta,X) = \\dfrac{2}{m}\\sum\\limits_{i=1}^m x_i^T (x_i \\theta-y_i) $$\n",
        "\n",
        "The Lipsitz constant of the gradient may be determined straight from the definition :\n",
        "\n",
        "$$\\|\\nabla_{\\theta_1}L(\\theta_1,X)-\\nabla_{\\theta_2}L(\\theta_2,X)\\| _2 \\le L\\|\\theta_1 - \\theta_2\\|_2, ~~~\\forall \\theta_1,\\theta_2 \\in \\mathbb{R^m}$$\n",
        "\n",
        "$$\\|\\nabla_{\\theta_1}L(\\theta_1,X)-\\nabla_{\\theta_2}L(\\theta_2,X)\\| _2 = \n",
        "\\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T (x_i (\\theta_1-\\theta_2))\\|_2 \\le \\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2 \\|\\theta_1-\\theta_2\\|_2$$\n",
        "\n",
        "$$\\Rightarrow  L \\le \\dfrac{2}{m}\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2$$\n",
        " \n",
        "$$As ~\\sum\\limits_{i=1}^m x_i^T x_i~~ is~ simmetric, $$\n",
        " \n",
        "$$\\|\\sum\\limits_{i=1}^m x_i^T x_i\\|_2 = \\lambda_{max}\\left(\\sum\\limits_{i=1}^mx_i^T x_i\\right)$$\n",
        " \n",
        "Also it is possible to find L from the following statement:\n",
        "\n",
        "$$\\|\\nabla_\\theta^2 L(\\theta, X)\\|_2^2  = \\lambda_{max}\\left(\\nabla_\\theta^2 L(\\theta, X)\\right) \\le L,~~~\\forall \\theta \\in \\mathbb{R^m}$$ \n",
        "\n",
        "$$\\nabla_{\\theta}^2 L(\\theta,X) = \\dfrac{2}{m}\\sum\\limits_{i=1}^mx_i^T x_i$$\n"
      ]
    },
    {
      "metadata": {
        "id": "lIY_AAMrqZw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss(reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kAIMHdEm_0Hj",
        "colab_type": "code",
        "outputId": "691a57dd-3cb6-409e-c598-71ef6ff78f35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# estimate the Lipsitz constant of the gradient\n",
        "\n",
        "def mse_hessian(X):\n",
        "    hess = 0\n",
        "    for x in X:\n",
        "        hess += 2 / X.size(0) * torch.ger(x, x)\n",
        "    return hess    \n",
        "  \n",
        "hess = mse_hessian(X_train)\n",
        "L = np.max(np.linalg.eigvals(hess.numpy())) + 1e-2\n",
        "\n",
        "print(\"the Lipsitz constant of the gradient does not exceed {:.3}\".format(L))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the Lipsitz constant of the gradient does not exceed 2.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s2CrOMinf1Zz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ]
    },
    {
      "metadata": {
        "id": "ANbit2lVI-dG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, n_columns):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(n_columns))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x @ self.weights\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4Cqx_BIf91i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, X_train, y_train, n_epochs=1, batch_size=4, print_every=1, X_val=None, y_val=None):\n",
        "    model.to(device)\n",
        "    tr_losses, val_losses, lips, grad = ([] for i in range(4))\n",
        "    batch_per_ep = X_train.size(0) // batch_size\n",
        "    for ep in range(n_epochs):\n",
        "        model.train()\n",
        "        for i in range(batch_per_ep):\n",
        "            inputs, y = Variable(X_train[i*batch_size:(i+1)*batch_size]).to(device), Variable(y_train[i*batch_size:(i+1)*batch_size]).to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, y)\n",
        "            tr_losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            if optimizer.__class__.__name__ != 'SUG':\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                def closure():\n",
        "                    optimizer.zero_grad()\n",
        "                    upd_outputs = model(inputs)\n",
        "                    upd_loss = criterion(upd_outputs, y) \n",
        "                    upd_loss.backward()\n",
        "                    return upd_loss\n",
        "                optimizer.step(loss, closure)\n",
        "                lips.append(optimizer.get_lipsitz_const())\n",
        "                grad.append(optimizer.get_sq_grad)\n",
        "                \n",
        "\n",
        "        model.zero_grad()\n",
        "        model.eval()\n",
        "        X, y = Variable(X_val).to(device), Variable(y_val).to(device)\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y) \n",
        "        val_losses.append(loss.item())\n",
        "        if ep % print_every == 0:\n",
        "            print(\"Epoch: {}, training loss: {}, validation loss: {}\".format(ep, sum(tr_losses[-batch_per_ep:])/batch_per_ep, val_losses[-1]))\n",
        "        \n",
        "    return tr_losses, val_losses, lips, grad            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cutdudpZ2IHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lrs = [0.1, 0.01, 0.001]\n",
        "n_epochs = 1\n",
        "tr_loss = {}\n",
        "tr_loss['sgd'] = {}\n",
        "val_loss = {}\n",
        "val_loss['sgd'] = {}\n",
        "criterion = nn.MSELoss(reduction=\"mean\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el_8C3JW59TY",
        "colab_type": "code",
        "outputId": "ab53843d-b1f5-4bf5-e535-02205adb9a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "for lr in lrs:\n",
        "    print(\"SGD lr={}\".format(lr))\n",
        "    model = LinearRegressionModel(n_columns)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "    tr_loss['sgd'][lr], val_loss['sgd'][lr], lips, grad = train(model, optimizer, X_train, y_train, n_epochs=n_epochs, batch_size=256, X_val=X_val, y_val=y_val, print_every=1)\n",
        "    #print(\"\\nTrained weights: \\n{}\\n\".format(list(model.parameters())[0].cpu().detach().numpy()))\n",
        "    print(\"Test score: {:.4}\\n\\n\".format(criterion(model(X_test.to(device)), y_test.to(device)).item()))\n",
        "    states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sgd'][lr],\n",
        "            'val_loss' : val_loss['sgd'][lr],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad\n",
        "        }\n",
        "    torch.save(states, './LINREG/lr_'+str(lr))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD lr=0.1\n",
            "Epoch: 0, training loss: 3.692259731586308, validation loss: 0.09422596544027328\n",
            "Test score: 0.09341\n",
            "\n",
            "\n",
            "SGD lr=0.01\n",
            "Epoch: 0, training loss: 3.5720966252215636, validation loss: 0.09422600269317627\n",
            "Test score: 0.09341\n",
            "\n",
            "\n",
            "SGD lr=0.001\n",
            "Epoch: 0, training loss: 3.57254397454264, validation loss: 0.09422596544027328\n",
            "Test score: 0.09341\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uHGhmvDt9VjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "984d9b1e-b15e-458f-8551-f09c14ecb309"
      },
      "cell_type": "code",
      "source": [
        "print(\"ADAM\")\n",
        "lr = 0.01\n",
        "model = LinearRegressionModel(n_columns)\n",
        "sgd = optim.Adam(model.parameters(), lr=lr)\n",
        "tr_loss['adam'], val_loss['adam'], lips, grad = train(model, sgd, X_train, y_train, n_epochs=1, batch_size=16, X_val=X_val, y_val=y_val, print_every=1)\n",
        "#print(\"\\nTrained weights: \\n{}\\n\".format(list(model.parameters())[0].cpu().detach().numpy()))\n",
        "print(\"Test score: {:.4}\".format(criterion(model(X_test.to(device)), y_test.to(device)).item()))\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['adam'],\n",
        "            'val_loss' : val_loss['adam'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad\n",
        "        }\n",
        "torch.save(states, './LINREG/lr_adam_' + str(lr))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ADAM\n",
            "Epoch: 0, training loss: 46.06552288995353, validation loss: 0.1441994607448578\n",
            "Test score: 0.1431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KO6h_SoaNYlF",
        "colab_type": "code",
        "outputId": "f4791ead-0d4b-4739-f411-baf4546ec910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"SUG\")\n",
        "model = LinearRegressionModel(n_columns)\n",
        "sgd = SUG(model.parameters(), l_0=8)\n",
        "tr_loss['sug'], val_loss['sug'], lips, grad = train(model, sgd, X_train, y_train, n_epochs=1, batch_size=16, X_val=X_val, y_val=y_val, print_every=1)\n",
        "#print(\"\\nTrained weights: \\n{}\\n\".format(list(model.parameters())[0].cpu().detach().numpy()))\n",
        "print(\"Test score: {:.4}\".format(criterion(model(X_test.to(device)), y_test.to(device)).item()))\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sug'],\n",
        "            'val_loss' : val_loss['sug'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad\n",
        "        }\n",
        "torch.save(states, './LINREG/lr_sug')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUG\n",
            "Epoch: 0, training loss: 0.8374266967736591, validation loss: 0.13306814432144165\n",
            "Test score: 0.1326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fuP0ZNPGnjKn",
        "colab_type": "code",
        "outputId": "df6b6919-1021-45b9-c9ce-5d0a26d8ee2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Trained weights:\\n{}\\n\".format(list(lr_model.coef_)))\n",
        "\n",
        "print(\"{:.4}\".format(criterion(torch.tensor(lr_model.predict(X_test)), y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trained weights:\n",
            "[5.0007753, 4.999613, 4.9998965, 5.000149, 5.0002346, 4.9998903, 5.0000505, 5.000227, 4.9995956, 4.999251, 5.000267, 4.999912, 5.000618, 5.000696, 5.00017, 4.9995766, 5.0010047, 5.0004873, 4.999726, 5.0000296, 4.9996996, 4.99969, 5.0003996, 5.0006623, 4.999458, 4.9998875, 5.0000353, 5.0003586, 4.999851, 5.000312, 4.999714, 4.9994636, 4.9998403, 5.0004435, 4.999973, 5.0003233, 5.0004697, 4.9998035, 4.9999256, 4.9999447, 5.000782, 5.0002737, 5.000174, 5.0003986, 5.0002728, 4.999854, 5.000162, 4.999118, 4.999841, 5.0001917, 5.0002103, 4.999758, 5.0000906, 5.00067, 4.9996786, 5.0003104, 5.000347, 4.999529, 5.000384, 4.999915, 4.99949, 5.000249, 4.999715, 5.000318, 4.9993095, 4.999655, 4.9990406, 4.999196, 5.000603, 5.000493, 5.000471, 4.999602, 4.9997187, 4.99999, 5.0003824, 4.9997807, 5.000211, 4.999608, 5.0002136, 4.9993877, 4.9996705, 5.0005617, 4.998989, 4.999974, 5.0003223, 4.9996195, 5.0010633, 5.000208, 5.0004177, 5.000323, 4.9999447, 4.9996486, 4.999584, 4.9999084, 4.9995146, 5.0001845, 4.9998884, 5.000203, 5.0000434, 4.999525]\n",
            "\n",
            "0.009865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ICfq7JBgVhcU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}