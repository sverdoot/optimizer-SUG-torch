{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sverdoot/optimizer-SUG-torch/blob/master/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_ni0nMDcukyV",
        "outputId": "8ac44723-fbd2-4c7a-9b93-e9735593874f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p-4vBJwaukwk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/Optimization project\")\n",
        "os.getcwd()\n",
        "\n",
        "file_path = \"./MNIST\"\n",
        "\n",
        "try:\n",
        "    os.stat(file_path)\n",
        "except:\n",
        "    os.mkdir(file_path)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QzSdhyDLASJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sug import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fiSHwkvZumRd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p2Mo1UdyumT9",
        "colab_type": "code",
        "outputId": "f14053f0-1237-42a4-f9ee-f5c16460b496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "W6Lk5gvGu1Yc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "aSGMNymTW5L8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rk3Q_Jku2by",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "valid_dataset = torchvision.datasets.MNIST(root='/data', train=True, \n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bYLS8LcHWv7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "valid_size=0.15\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "               batch_size=batch_size, sampler=train_sampler,\n",
        "               num_workers=2)\n",
        "\n",
        "validloader = torch.utils.data.DataLoader(valid_dataset, \n",
        "               batch_size=batch_size, sampler=valid_sampler,\n",
        "               num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7j9cjtv5u57l",
        "colab_type": "code",
        "outputId": "2d6a7e4b-aa09-49fd-9dbd-d0e3f65ff814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "def show_batch(batch):\n",
        "    im = torchvision.utils.make_grid(batch)\n",
        "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
        "    \n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print('Labels: ', labels)\n",
        "print('Batch shape: ', images.size())\n",
        "show_batch(images)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Labels:  tensor([9, 6, 7, 7])\n",
            "Batch shape:  torch.Size([4, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAB6CAYAAACr63iqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEDhJREFUeJzt3X2QVFV6x/HvI4LiKkFEUQFfQ7ll\nrKxsoeUqpdbgGl9WWTFugetKKihSvgcLhfDHGl9KovISTbIWruziloUhIohINGaCYEpXGbMWi6CC\nq7yJjJS4uvEVffLHvX05w3RPv3fPvfP7VE3N06dv9z13bnM4fc65zzV3R0REsmOfZldARERqSw27\niEjGqGEXEckYNewiIhmjhl1EJGPUsIuIZIwadhGRjKmqYTez88zsLTPbaGZTa1UpERGpnFV6gZKZ\n9QLeBn4IbAVWA+PcfV3tqiciIuXat4rXngpsdPc/AJjZ48BooGDDbma6zFVEpHw73f3QUjeuZihm\nMLAleLw1LuvAzCaaWZuZtVWxLxGRnmxTORtX02MvibvPBeaCeuwiIo1QTY99GzA0eDwkLhMRkSaq\npmFfDQwzs2PNrA8wFlham2qJiEilKh6KcffdZnY98BzQC5jn7m/UrGYiIlKRipc7VrQzjbGLiFTi\nNXcfUerGuvJURCRj1LCLiGSMGnYRkYxRwy4ikjFq2EVEMkYNu4hIxqhhFxHJmLrnipGey8wA2L17\nd1LWv3//JP70008bXieRnkA9dhGRjFGPXWrqlFNOSeKXXnoJgG+//TYpUy+9NN98802Xzz/77LNJ\nfOGFF9a7OpIy6rGLiGSMGnYRkYzRUIzU1JgxYzqV9evXrwk1SZ+rr746ib/66qsk3rlzZ6dtW1pa\nkvjrr79O4t69e9epdpIm6rGLiGSMGnYRkYxRPnapqXBYoL29HYDBgzvd41zyCP92c+bMSeIpU6Z0\n+boFCxYk8cUXX5zEy5cvB+Cyyy6rVRW7tbAtC6+dGDt2LACLFi1qeJ1qSPnYRUR6MjXsIiIZ0+NW\nxQwdOjSJJ02aBMCyZcuSspdffrnqfZxxxhkAvP/++0nZu+++W/X7dlf33HNP3vLw7yrFffbZZ0lc\nbPglNG7cuCSePXt2El9//fW1qVg3tnLlyiQOh1/C+OabbwY6Xii3ePHiBtSueYr22M1snpm1m9na\noGyAmT1vZhvi3wfXt5oiIlKqopOnZnYm8CfgUXc/KS67F/jI3WeY2VTgYHe/rejOmjR5umnTpiQe\nMmRIl9tu3rw5iY877rgkLvZ3mj9/fhJfeeWVwJ5L6mFPLz6Lwkm/kNZUl+a5554D4L333kvKrrnm\nmqrfN3deTj/99KRs9erVVb9vdxJ+9vbZZ08/Neyd58rzlQEceeSRSbxjx45O+wgn/8NvAvm2raPa\nTp66+yrgo72KRwO5lmw+8OOSqyciInVV6Rj7IHffHscfAIMKbWhmE4GJFe5HRETKVPXkqbt7V0Ms\n7j4XmAuNHYoJs+Nt3LgxiXv16tVp2/Hjx+d9j7fffjuJhw0b1uX+rrjiik6vy/Lwi9ROLj3A1KlT\nm1yT9LjvvvuAjkMj++67pznLV15o23AIrNi2Yfnll18OdM+J2EqXO+4wsyMA4t/ttauSiIhUo9KG\nfSmQ6+aOB56qTXVERKRaRYdizGwBcDYw0My2Aj8HZgALzWwCsAn4ST0rWY7jjz++U9kJJ5zQ5WvC\nFS2llOcMHz48b/mSJUu6fJ1Ia2trEn/yyScAzJw5sy77Wrt2bfGNUmby5MlA4ZUu4fBJrjxfWS22\n7Y6KNuzuPq7AU6NqXBcREamB7vtfjoiIVCRzKQUaeRn7qlWr8pbfdlvRa7UyK/xqLIWdeeaZSVzL\n1TBhpseczz//vGbv30yXXHJJEuc+Z7VYFVPptt35s64eu4hIxmSux17vCY0XX3wxiQ844IC67iuN\n2traml2FbmvChAl5y2s5aTpy5MgkfvPNN2v2vt1BuF68nEnOXD72cpKAFUpVkJbJ0+5bMxERqYga\ndhGRjMncUEy9JjSeeeYZAI466qik7KOP9uRGGzBgQF32mwaFvqpKRw899FAS33///TV734cffjiJ\nDz/88CTOZRnNolxGxjAdwAMPPJDE1S5g0OSpiIh0K2rYRUQyJnPfm3M3Fdi5c2dSFmZm3LBhQ8nv\nFa6Ayb1vmJT/nHPOSeJHH320/MpmRPiVdODAgV1uu23btryvC29ZmCXhSphZs2Yl8bRp02q2jwsu\nuCCJ16xZk8QrVqyo2T66m9xNLvr27VuX9097SoHuWzMREamIGnYRkYzJ3FDMrl27AHjhhReSsvBC\njbvuuiuJc7Po4UqXMCVBnz59kjh3MdKXX36ZlBUbduiJwr9lvnuhFrovZb5t58yZk8RTpkypVRUb\n6t57703iQw45pGbvu2XLliQ+7LDDknjSpEk120dPVM4NPLQqRkREGsbcG3a3uobeGi/09NNPJ3E4\n0ZRP2GMfPXp0l9uGt9SbN29eEue7/V6W5ettA7S3RzfWCu/yXsj06dMBuP322/M+37t378oql1GF\n/ub6O1Und0vNQnnew+sPGpzs7zV3H1Hqxuqxi4hkjBp2EZGMydzkaT4XXXRREptZEs+YMQPomA+7\nnKGpcHKvpzjrrLOA0tbt33jjjSW/79133w1AS0tLUhbmLJfI9u3bO5UtXbq0CTXJjpUrVyZxvjzv\ntUxV0ChFe+xmNtTMVpjZOjN7w8xuissHmNnzZrYh/n1w/asrIiLFlDIUsxu4xd1PBE4DrjOzE4Gp\nQKu7DwNa48ciItJkZa+KMbOngH+Of8529+1mdgTwgrufUOS1TVkVUy+5GfS9ZXlVzIMPPgjAmDFj\nkrKFCxcm8bnnnpvEBx54IABHH310ye/f2tqaxOFQTJZXe+RWbfXr1y8py6WwqLXNmzcn8bp165I4\nHK7safLdVCNcFZO7UQfAokWLGlexjspaFVPWGLuZHQMMB14BBrl7bsDvA2BQgddMBCaWsx8REalc\nyatizOxAYBFws7t/Ej7nUbc/b2/c3ee6+4hy/rcREZHKldRjN7PeRI36Y+7+ZFy8w8yOCIZi2utV\nyTS49dZbm12Fhrjhhhs6/N7bqFGjknjJkiVlv384/BJm6EybSy+9FOi4ciq8CUa1rrrqqiQOh1Sk\nNLnUAZA/fUBY1sThl4qVsirGgEeA9e4+K3hqKZC79HI88FTtqyciIuUqpcd+BvAz4Pdm9npc9vfA\nDGChmU0ANgE/qU8Vu6/cJfMA/fv3b2JNuo9wcm7//fcHOk5OFZoEve666zqVhfnL0ybXiw4nkwtZ\nv359p7JCKQNySezmz59fRe1k8uTJSZwvfUC4dj2Nijbs7v4/gBV4elSBchERaRKlFBARyZgekVKg\nXj7++OMkDrNC9mThrQdzE8phTvLwuolwgirniy++SOJwgitt8g2vVCrMcZ8vpYCUJl/qAMifPiAt\nqQMKUY9dRCRj1LCLiGSMhmKqEK5L7s53LG+W2bNnA3tSCwDccccdXb7moIMOqmudurMFCxYkcVtb\nWxJr+KU2wjQN4b/X8NZ3r776akPrVC9qjUREMkYNu4hIxmgopgrhbHqYmU86uvPOO/PG0lGYGmDc\nuHFNrEk25UsdsHd5uFomzdRjFxHJGPXYq9CnT58kXr58eRIPHDgQgF27djW8TpJe+jZTHzNnzgQ6\n9tILTZ6OHDkSgMWLFzeodvWhHruISMaoYRcRyRgNxVQhXPPa0tKSxLmJVA3FiDTfLbfcAsC1116b\nlBWaPM2X5iKN1GMXEckYNewiIhmjoZgqhHcvnzdvXhJv2rSpGdURkS707du32VVoGPXYRUQyRg27\niEjGWHjjg7wbmO0PrAL2Ixq6ecLdf25mxwKPA4cArwE/c/evirxX1zsTEZF8XnP3EaVuXEqP/Uug\nxd2/B5wMnGdmpwH/CMx29z8HdgETKqmtiIjUVtGG3SN/ih/2jn8caAGeiMvnAz+uSw1FRKQsJY2x\nm1kvM3sdaAeeB94BPnb33Gr+rcDg+lRRRETKUVLD7u7fuPvJwBDgVOC7pe7AzCaaWZuZtRXfWkRE\nqlXWqhh3/xhYAfwA6G9muXXwQ4BtBV4z191HlDPwLyIilSvasJvZoWbWP477Aj8E1hM18H8dbzYe\neKpelRQRkdKVcuXpEcB8M+tF9B/BQndfZmbrgMfN7C7gd8AjdayniIiUqOg69pruzOxD4P+AnQ3b\naWMNRMeWRjq2dOpJx3a0ux9a6osb2rADmFlbVsfbdWzppGNLJx1bYUopICKSMWrYRUQyphkN+9wm\n7LNRdGzppGNLJx1bAQ0fYxcRkfrSUIyISMaoYRcRyZiGNuxmdp6ZvWVmG81saiP3XWtmNtTMVpjZ\nOjN7w8xuissHmNnzZrYh/n1ws+taiTjx2+/MbFn8+FgzeyU+d/9mZn2aXcdKmFl/M3vCzN40s/Vm\n9oMMnbO/iz+La81sgZntn9bzZmbzzKzdzNYGZXnPk0UeiI9xjZl9v3k1L67Asd0XfybXmNni3NX+\n8XPT4mN7y8z+qpR9NKxhj69c/RfgfOBEYJyZndio/dfBbuAWdz8ROA24Lj6eqUCruw8DWuPHaXQT\nUeqInKzk3/8n4Fl3/y7wPaJjTP05M7PBwI3ACHc/CegFjCW95+3XwHl7lRU6T+cDw+KficAvGlTH\nSv2azsf2PHCSu/8l8DYwDSBuU8YCfxG/5l/jtrRLjeyxnwpsdPc/xHdaehwY3cD915S7b3f3/43j\nT4kaiMFExzQ/3iyVeerNbAhwIfDL+LGRgfz7ZvZnwJnE6S/c/as4sV3qz1lsX6BvnJzvAGA7KT1v\n7r4K+Giv4kLnaTTwaHzviN8SJSg8ojE1LV++Y3P3/wzSoP+WKLEiRMf2uLt/6e7vAhuJ2tIuNbJh\nHwxsCR5nJoe7mR0DDAdeAQa5+/b4qQ+AQU2qVjXmALcC38aPDyEb+fePBT4EfhUPM/3SzL5DBs6Z\nu28D7gc2EzXofyS6ZWUWzltOofOUtbblb4H/iOOKjk2Tp1UyswOBRcDN7v5J+JxHa0lTtZ7UzH4E\ntLv7a82uSx3sC3wf+IW7DyfKW9Rh2CWN5wwgHm8eTfSf15HAd+j8dT8z0nqeijGz6UTDvI9V8z6N\nbNi3AUODxwVzuKeFmfUmatQfc/cn4+Idua+B8e/2ZtWvQmcAF5vZe0TDZS1E49Il5d/v5rYCW939\nlfjxE0QNfdrPGcA5wLvu/qG7fw08SXQus3Decgqdp0y0LWb2N8CPgJ/6nguMKjq2Rjbsq4Fh8Sx9\nH6IJgaUN3H9NxePOjwDr3X1W8NRSovz0kMI89e4+zd2HuPsxROfov939p2Qg/767fwBsMbMT4qJR\nwDpSfs5im4HTzOyA+LOZO7bUn7dAofO0FLgyXh1zGvDHYMgmFczsPKLhz4vd/bPgqaXAWDPbz8yO\nJZogfrXoG7p7w36AC4hmfN8Bpjdy33U4lpFEXwXXAK/HPxcQjUe3AhuA/wIGNLuuVRzj2cCyOD4u\n/kBtBP4d2K/Z9avwmE4G2uLztgQ4OCvnDPgH4E1gLfAbYL+0njdgAdFcwddE37QmFDpPgBGtuHsH\n+D3RyqCmH0OZx7aRaCw915Y8FGw/PT62t4DzS9mHUgqIiGSMJk9FRDJGDbuISMaoYRcRyRg17CIi\nGaOGXUQkY9Swi4hkjBp2EZGM+X8Y0ftzo99TFwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "THk9uDZku-gM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "pnS_0OICbHF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "H6micuYGbGOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LR, self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = F.log_softmax(self.linear1(x.view(batch_size, -1)), -1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4rjRazJa-S6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###   FC"
      ]
    },
    {
      "metadata": {
        "id": "uh3HnmUmu96Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FC, self).__init__()\n",
        "        self.linear1 = nn.Linear(28*28, 256)\n",
        "        self.linear2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_relu = F.relu(self.linear1(x.view(batch_size, -1)))\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xz0yOQOSbArU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conv"
      ]
    },
    {
      "metadata": {
        "id": "QKUu1qZta7el",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(3*3*64, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        #x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = x.view(-1,3*3*64 )\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-SSTdIH_vKV5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "nMttjLlZrbcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def model_step(model, optimizer, criterion, inputs, labels):\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    if model.training:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "    if optimizer.__class__.__name__ != 'SUG':\n",
        "        optimizer.step()\n",
        "    else:\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            upd_outputs = model(inputs)\n",
        "            upd_loss = criterion(upd_outputs, labels)\n",
        "            upd_loss.backward()\n",
        "            return upd_loss\n",
        "\n",
        "        optimizer.step(loss, closure)\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMgATVQnvIiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, criterion, optimizer, n_epochs=2, validloader=None, eps=1e-5, print_every=1):\n",
        "    tr_loss, val_loss, lips, times, grad, acc = ([] for i in range(6))\n",
        "    start_time = time.time()\n",
        "    model.to(device=device)\n",
        "    for ep in range(n_epochs):\n",
        "        model.train()\n",
        "        i = 0\n",
        "        for i, data in enumerate(trainloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs).to(device=device), Variable(labels).to(device=device)\n",
        "\n",
        "            tr_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
        "            if optimizer.__class__.__name__ == 'SUG':\n",
        "                lips.append(optimizer.get_lipsitz_const())\n",
        "                grad.append(optimizer.get_sq_grad)\n",
        "        times.append(time_since(start_time))\n",
        "        if ep % print_every == 0:\n",
        "            print(\"Epoch {}, training loss {}, time passed {}\".format(ep, sum(tr_loss[-i:]) / i, time_since(start_time)))\n",
        "\n",
        "        if validloader is None:\n",
        "            continue\n",
        "        model.zero_grad()\n",
        "        model.eval()\n",
        "        j = 0\n",
        "        for j, data in enumerate(validloader):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
        "            val_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
        "        if ep % print_every == 0:\n",
        "            print(\"Validation loss {}\".format(sum(val_loss[-j:]) / j))\n",
        "        \n",
        "    return tr_loss, times, val_loss, lips, grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sz4svVB3vNyZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_every = 4\n",
        "n_epochs = 10\n",
        "tr_loss = {}\n",
        "tr_loss['sgd'] = {}\n",
        "val_loss = {}\n",
        "val_loss['sgd'] = {}\n",
        "lrs = [0.05, 0.01, 0.005]\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6jCFM66X7ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concat_states(state1, state2):\n",
        "    states = {\n",
        "            'epoch': state1['epoch'] + state2['epoch'],\n",
        "            'state_dict': state2['state_dict'],\n",
        "            'optimizer': state2['optimizer'],\n",
        "            'tr_loss' : state1['tr_loss'] + state2['tr_loss'],\n",
        "            'val_loss' : state1['val_loss'] + state2['val_loss'],\n",
        "            'lips' : state1['lips'] + state2['lips'],\n",
        "            'grad' : state1['grad'] + state2['grad'],\n",
        "            #'times' : state1['times'] + list(map(lambda x: x + state1['times'][-1],state2['times']))\n",
        "             'times' : state1['times'] + state2['times']\n",
        "             }\n",
        "    return states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "98EHtawSzoTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###LR"
      ]
    },
    {
      "metadata": {
        "id": "hTMWs2dYznyO",
        "colab_type": "code",
        "outputId": "01e33386-4143-4fe1-ead2-ef3ccd13cea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "for lr in lrs:\n",
        "  model = LR()\n",
        "  print(\"SGD  lr={}, momentum=0. :\".format(lr))\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
        "  tr_loss['sgd'][lr], times, val_loss['sgd'][lr], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "  states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sgd'][lr],\n",
        "            'val_loss' : val_loss['sgd'][lr],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "             }\n",
        "  torch.save(states, './MNIST/LR_' + str(lr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD  lr=0.05, momentum=0. :\n",
            "Epoch 0, training loss 1.1831175889721082, time passed 0m 38s\n",
            "Validation loss 1.7346817483001529\n",
            "Epoch 4, training loss 0.9851548817687289, time passed 3m 6s\n",
            "Validation loss 0.9243420956190447\n",
            "Epoch 8, training loss 0.9310133712944944, time passed 5m 36s\n",
            "Validation loss 1.1689711035859476\n",
            "SGD  lr=0.01, momentum=0. :\n",
            "Epoch 0, training loss 0.41623996844117783, time passed 0m 32s\n",
            "Validation loss 0.32225607072639145\n",
            "Epoch 4, training loss 0.32428377992083196, time passed 2m 56s\n",
            "Validation loss 0.3230409316380277\n",
            "Epoch 8, training loss 0.31112885381527805, time passed 5m 22s\n",
            "Validation loss 0.3265203488063716\n",
            "SGD  lr=0.005, momentum=0. :\n",
            "Epoch 0, training loss 0.4129749693272001, time passed 0m 30s\n",
            "Validation loss 0.34086438555364335\n",
            "Epoch 4, training loss 0.29709475929701606, time passed 2m 55s\n",
            "Validation loss 0.29196425313139596\n",
            "Epoch 8, training loss 0.2841329223229156, time passed 5m 20s\n",
            "Validation loss 0.2948868198833449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b1CbtL-F0xaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l_0 = 2\n",
        "model = LR()\n",
        "optimizer = SUG(model.parameters(), l_0=l_0, momentum=0.)\n",
        "tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sug'],\n",
        "            'val_loss' : val_loss['sug'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "         }\n",
        "torch.save(states, './MNIST/LR_sug')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJ24b8FFpIQU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### FC"
      ]
    },
    {
      "metadata": {
        "id": "zBxqCh7YEOok",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlkSl5x-vPRW",
        "colab_type": "code",
        "outputId": "0c739f08-c0ac-4326-b69d-870d51daa4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "for lr in lrs:\n",
        "  model = FC()\n",
        "  print(\"SGD  lr={}, momentum=0. :\".format(lr))\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
        "  tr_loss['sgd'][lr], times, val_loss['sgd'][lr], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "  states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sgd'][lr],\n",
        "            'val_loss' : val_loss['sgd'][lr],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "             }\n",
        "  torch.save(states, './MNIST/FC_' + str(lr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD  lr=0.05, momentum=0. :\n",
            "Epoch 0, training loss 0.2366863005871361, time passed 0m 29s\n",
            "Validation loss 0.11924165910008008\n",
            "Epoch 4, training loss 0.036543241790046185, time passed 2m 44s\n",
            "Validation loss 0.02659412257774187\n",
            "Epoch 8, training loss 0.009252302931265622, time passed 4m 57s\n",
            "Validation loss 0.007337128495894416\n",
            "SGD  lr=0.01, momentum=0. :\n",
            "Epoch 0, training loss 0.390318907132868, time passed 0m 28s\n",
            "Validation loss 0.21850812987732113\n",
            "Epoch 4, training loss 0.07997714770486358, time passed 2m 43s\n",
            "Validation loss 0.06550830985610355\n",
            "Epoch 8, training loss 0.04263166229681311, time passed 4m 59s\n",
            "Validation loss 0.0345691220396728\n",
            "SGD  lr=0.005, momentum=0. :\n",
            "Epoch 0, training loss 0.49476544181125587, time passed 0m 28s\n",
            "Validation loss 0.2863557395032906\n",
            "Epoch 4, training loss 0.13017870467198542, time passed 2m 44s\n",
            "Validation loss 0.11114349794313609\n",
            "Epoch 8, training loss 0.0772063676231337, time passed 4m 57s\n",
            "Validation loss 0.06647039221731384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ODga9MkTvnRl",
        "colab_type": "code",
        "outputId": "e1949814-6533-4078-9b5e-cf193b98398f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "l_0 = 2\n",
        "model = FC()\n",
        "optimizer = SUG(model.parameters(), l_0=l_0, momentum=0.)\n",
        "tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sug'],\n",
        "            'val_loss' : val_loss['sug'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "         }\n",
        "torch.save(states, './MNIST/FC_sug')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, training loss 0.2193387469148363, time passed 0m 59s\n",
            "Validation loss 0.11775196808452021\n",
            "Epoch 4, training loss 0.03766450093525402, time passed 5m 24s\n",
            "Validation loss 0.030662210186940927\n",
            "Epoch 8, training loss 0.017794447657136097, time passed 9m 37s\n",
            "Validation loss 0.013901273798232293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2x_ieelg84rE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ]
    },
    {
      "metadata": {
        "id": "PVXwssDp83x6",
        "colab_type": "code",
        "outputId": "c9eb5444-4fb2-48d1-a8ff-f2359ee858fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "for lr in lrs:\n",
        "  model = CNN()\n",
        "  print(\"SGD  lr={}, momentum=0. :\".format(lr))\n",
        "  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
        "  tr_loss['sgd'][lr], times, val_loss['sgd'][lr], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "  states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sgd'][lr],\n",
        "            'val_loss' : val_loss['sgd'][lr],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "             }\n",
        "  torch.save(states, './MNIST/CNN_' + str(lr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD  lr=0.01, momentum=0. :\n",
            "Epoch 0, training loss 0.30792822736959785, time passed 0m 50s\n",
            "Validation loss 0.13387587281877472\n",
            "Epoch 4, training loss 0.0733286956256355, time passed 4m 53s\n",
            "Validation loss 0.07147482255601184\n",
            "SGD  lr=0.005, momentum=0. :\n",
            "Epoch 0, training loss 0.37557755943024357, time passed 0m 55s\n",
            "Validation loss 0.1362349701132594\n",
            "Epoch 4, training loss 0.07144774527515312, time passed 4m 59s\n",
            "Validation loss 0.06511574609441027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YTYmncEz87xM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l_0 = 2\n",
        "model = CNN()\n",
        "optimizer = SUG(model.parameters(), l_0=l_0, momentum=0.)\n",
        "tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
        "states = {\n",
        "            'epoch': n_epochs,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'tr_loss' : tr_loss['sug'],\n",
        "            'val_loss' : val_loss['sug'],\n",
        "            'lips' : lips,\n",
        "            'grad' : grad,\n",
        "            'times' : times\n",
        "         }\n",
        "torch.save(states, './MNIST/CNN_sug')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H8y30s5VE4px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}