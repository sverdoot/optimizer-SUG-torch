{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SUG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sverdoot/optimizer-SUG-torch/blob/master/SUG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HVQTd54M8maE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stochastic  Universal Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "-xijwbH88xl8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###**Adaptive Stochastic Gradient Method (Spokoiny's practical variant)**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "$\\textbf{Input}$: lower estimate for the variance of the gradient $D_0 \\le D$, accuracy $0 <                        \n",
        "\\epsilon< \\frac{D_0}{L}$, starting point $x_0 \\in Q$, initial guess $L_{-1} > 0$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        " $\\textbf{for }${$k=0,1,...$}\n",
        " \n",
        "$~~~~~~~~~~~~$Set $i_k=0$. Set $r^k = \\lceil \\frac{2 D_0}{L_{k-1}} {\\varepsilon}\\rceil$, generate i.i.d. $\\xi^i_K, ~i = 1,\\dots, r^k$\n",
        "\t    \n",
        "$~~~~~~~~~~~~$$\\textbf{repeat }$\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~~~~$Set $L_k = 2 ^{i_k-1}L_{k-1}$\n",
        "\t\t\n",
        "$~~~~~~~~~~~~~~~~~~~~$Calculate $\\tilde{g}(x_k) = \\frac{1}{r^k}\\sum_{i=1}^{r^k}\\nabla f(x_k, \\xi^i_k)$.\n",
        "\t\t\n",
        "$~~~~~~~~~~~~~~~~~~~~$Calculate $w_k = x_k - \\frac{1}{2 L_k}\\tilde{g}(x_k)$.\n",
        "\t\t\n",
        "$~~~~~~~~~~~~~~~~~~~~$Calculate $\\tilde{f}(x_k) = \\frac{1}{r_k}\\sum_{i=1}^{r^k}f(x_k, \\xi^i_k)$ and   $\\tilde{f}(w_k) = \\frac{1}{r^k}\\sum_{i=1}^{r^k}f(w_k, \\xi^i_k)$.\n",
        "\t\t\n",
        "$~~~~~~~~~~~~~~~~~~~~$Set $i_k = i_k + 1$.\n",
        "\n",
        "$~~~~~~~~~~~~$$\\textbf{until }$ $~~~~\\tilde{f}(w_k) \\le \\tilde{f}(x_k) + \\langle\\tilde{g}(x_k), w_k - x_k\\rangle + \\frac{2 L_k}{2}\\|w_k - x_k\\|_2^2 + \\frac{\\epsilon}{10}$.\n",
        "\t\t\n",
        "$~~~~~~~~~~~~$Set $x_{k+1} = w_k,~k=k+1$.\n",
        "\t\n",
        "$\\textbf{endfor}$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "txd-VwRxtorA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let $g(x)$ be a stochastic gradient of the function being minimized.\n",
        "\n",
        "In every iteration we have to check if the following inequality is satisfied:\n",
        "\n",
        "$f(w) \\le f(x) + \\langle g(x), w-x \\rangle + \\dfrac{2L}{2}\\|w-x\\|^2_2 + \\dfrac{\\epsilon}{10}$\n",
        "\n",
        "Substituting $w$ with the its definition expression,\n",
        "\n",
        "$w = x - \\frac{1}{2L}g(x)$\n",
        "\n",
        "We will get\n",
        "\n",
        "$f(w) \\le f(x) - \\dfrac{1}{2L}\\|g(x)\\|^2_2 + \\dfrac{2L}{2}\\dfrac{1}{4L^2}\\|g(x)\\|^2_2 + \\dfrac{\\epsilon}{10}$\n",
        "\n",
        "or \n",
        "\n",
        "$f(w) \\le f(x) - \\dfrac{1}{4L}\\|g(x)\\|^2_2 +  \\dfrac{\\epsilon}{10}$\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "WX2_4b66vsXa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In case of neural networks it is not obvious what $g(x)$ actually is.\n",
        "\n",
        "If we consider f(x) to be a function of a range of matrces and vectors:\n",
        "\n",
        "$f(x)  = f(W_1, b_1, \\dots, W_n, b_n),$\n",
        "\n",
        "compute its gradient\n",
        "\n",
        "$$df(W_1,b_1,\\dots,W_n, b_n) = \\sum\\limits_{i=1}^n(\\dfrac{\\partial f}{\\partial W_i}dW_i+ \\dfrac{\\partial f}{\\partial b_i}db_i)(W_1,b_1,\\dots,W_n,b_n)$$\n",
        "\n",
        "The goal is to represent $df$ in this fashion:\n",
        "\n",
        "$$df(x) = \\langle g(x), dx \\rangle$$\n",
        "\n",
        "In this case $g(x)$ is the gradient.\n",
        "\n",
        "Let's notice that in case of $x$ is vector, $x \\in \\mathbb R^n, ~g(x) \\in \\mathbb R^n$\n",
        "\n",
        "$$\\langle g(x), x\\rangle = \\sum\\limits_{i=1}^n g_i(x) x_i $$\n",
        "\n",
        "and so we do if $X$ is a matrix: $X \\in Mat(n\\times m), ~g(X) \\in Mat(n\\times m)$\n",
        "\n",
        "$$\\langle g(X), X \\rangle = \\mathbf{tr}g(X)X = g(X) \\cdot X = \\sum_{i=1}^n\\sum_{j=1}^mg_{ij}(X)X_{ij}$$\n",
        "\n",
        "That means we may consider $X$ as a vector $(x_{11},x_{12},\\dots,x_{1m},x{21},\\dots,x_{nm})$ of dimension $nm$ and the result will not change.\n",
        "\n",
        "Such reasoning  allows us to compute the second norm of the gradient in a following way:\n",
        "\n",
        "$$\\|g(x)\\|_2^2 = \\|g(W_1,b_1,\\dots,W_n,b_n)\\|_2^2 = \\sum\\limits_{i=1}^n \\left(g_{W_1}(x)\\cdot g_{W_1}(x) + \\langle g_{b_1}(x)  , g_{b_1}(x)\\rangle\\right)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "qntRpSwVvr1K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class SUG(Optimizer):\n",
        "    def __init__(self, params, l_0, d_0=0, prob=1., eps=1e-4, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if l_0 < 0.0:\n",
        "            raise ValueError(\"Invalid Lipsitz constant of gradient: {}\".format(l_0))\n",
        "        if d_0 < 0.0:\n",
        "            raise ValueError(\"Invalid disperion of gradient: {}\".format(d_0))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(L=l_0, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        self.Lips = l_0\n",
        "        self.prev_Lips = l_0\n",
        "        self.D_0 = d_0\n",
        "        self.eps = eps\n",
        "        self.prob = prob\n",
        "        self.start_param = params\n",
        "        self.upd_sq_grad_norm = None\n",
        "        self.sq_grad_norm = None\n",
        "        self.loss = torch.tensor(0.)\n",
        "        self.closure = None\n",
        "        super(SUG, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SUG, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def comp_batch_size(self):\n",
        "        \"\"\"Returns optimal batch size for given d_0, eps and l_0;\n",
        "\n",
        "        \"\"\"\n",
        "        return math.ceil(2 * self.D_0 * self.eps / self.prev_Lips)\n",
        "\n",
        "    def step(self, loss, closure):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            loss : current loss\n",
        "\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        self.start_params = []\n",
        "        self.loss = loss\n",
        "        self.sq_grad_norm = 0\n",
        "        self.closure = closure\n",
        "        for gr_idx, group in enumerate(self.param_groups):\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "            self.start_params.append([])\n",
        "            for p_idx, p in enumerate(group['params']):\n",
        "                self.start_params[gr_idx].append([p.data])\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                self.start_params[gr_idx][p_idx].append(p.grad.data)\n",
        "                d_p = self.start_params[gr_idx][p_idx][1]\n",
        "                p_ = self.start_params[gr_idx][p_idx][0]\n",
        "                self.sq_grad_norm += torch.sum(p.grad.data * p.grad.data)\n",
        "                \n",
        "                if weight_decay != 0:\n",
        "                   d_p.add_(weight_decay, p.data)\n",
        "                \n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.zeros_like(p.data)\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "                self.start_params[gr_idx][p_idx][1] = d_p\n",
        "        i = 0\n",
        "        difference = -1\n",
        "        while difference < 0:\n",
        "            self.Lips = max(self.prev_Lips * 2 ** (i - 1), 2.)\n",
        "            for gr_idx, group in enumerate(self.param_groups):\n",
        "                for p_idx, p in enumerate(group['params']):\n",
        "                    if p.grad is None:\n",
        "                        continue\n",
        "                    start_param_val = self.start_params[gr_idx][p_idx][0]\n",
        "                    start_param_grad = self.start_params[gr_idx][p_idx][1]\n",
        "                    p.data = start_param_val - 1/(2*self.Lips) * start_param_grad\n",
        "            difference, upd_loss = self.stop_criteria()\n",
        "            i += 1\n",
        "        self.prev_Lips = self.Lips\n",
        "\n",
        "        return self.Lips, i\n",
        "\n",
        "    def stop_criteria(self):\n",
        "        \"\"\"Checks if the Lipsitz constant of gradient is appropriate\n",
        "        \n",
        "           <g(x_k), w_k - x_k> + 2L_k / 2 ||x_k - w_k||^2 = - 1 / (2L_k)||g(x_k)||^2 + 1 / (4L_k)||g(x_k)||^2 = -1 / (4L_k)||g(x_k)||^2                \n",
        "        \"\"\"\n",
        "        cur_loss = self.loss.item()\n",
        "        upd_loss = self.closure().item()\n",
        "        major =  cur_loss - 1 / (4 * self.Lips) * self.sq_grad_norm\n",
        "        return major - upd_loss + self.eps / 10, upd_loss\n",
        "\n",
        "    def get_lipsitz_const(self):\n",
        "        \"\"\"Returns current Lipsitz constant of the gradient of the loss function\n",
        "        \"\"\"\n",
        "        return self.Lips\n",
        "    \n",
        "    def get_sq_grad(self):\n",
        "        \"\"\"Returns the current second norm of the gradient of the loss function \n",
        "           calculated by the formula\n",
        "           \n",
        "           ||f'(p_1,...,p_n)||_2^2 ~ \\sum\\limits_{i=1}^n ((df/dp_i) * (df/dp_i))(p1,...,p_n))\n",
        "           \n",
        "        \"\"\"\n",
        "        self.upd_sq_grad_norm = 0\n",
        "        for gr_idx, group in enumerate(self.param_groups):\n",
        "            for p_idx, p in enumerate(group['params']):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                self.upd_sq_grad_norm += torch.sum(p.grad.data * p.grad.data) \n",
        "        \n",
        "        return self.upd_sq_grad_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0n-kQv9IPXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}