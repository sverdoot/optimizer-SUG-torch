{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import optimizer\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBound(Optimizer):\n",
    "    \"\"\"AdaBound optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        final_lr: float >= 0. Final learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        gamma: float >= 0. Convergence speed of the bound function.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        weight_decay: Weight decay weight.\n",
    "        amsbound: boolean. Whether to apply the AMSBound variant of this\n",
    "            algorithm.\n",
    "    # References\n",
    "        - [Adaptive Gradient Methods with Dynamic Bound of Learning Rate]\n",
    "          (https://openreview.net/forum?id=Bkg3g2R9FX)\n",
    "        - [Adam - A Method for Stochastic Optimization]\n",
    "          (https://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond]\n",
    "          (https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, final_lr=0.1, beta_1=0.9, beta_2=0.999, gamma=1e-3,\n",
    "                 epsilon=None, decay=0., amsbound=False, weight_decay=0.0, **kwargs):\n",
    "        super(AdaBound, self).__init__(**kwargs)\n",
    "\n",
    "        if not 0. <= gamma <= 1.:\n",
    "            raise ValueError(\"Invalid `gamma` parameter. Must lie in [0, 1] range.\")\n",
    "\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "\n",
    "        self.final_lr = final_lr\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsbound = amsbound\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "        self.base_lr = float(lr)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        # Applies bounds on actual learning rate\n",
    "        step_size = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                          (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        final_lr = self.final_lr * lr / self.base_lr\n",
    "        lower_bound = final_lr * (1. - 1. / (self.gamma * t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (self.gamma * t))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsbound:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            # apply weight decay\n",
    "            if self.weight_decay != 0.:\n",
    "                g += self.weight_decay * K.stop_gradient(p)\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "\n",
    "            if self.amsbound:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                denom = (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                denom = (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            # Compute the bounds\n",
    "            step_size_p = step_size * K.ones_like(denom)\n",
    "            step_size_p_bound = step_size_p / denom\n",
    "            bounded_lr_t = m_t * K.minimum(K.maximum(step_size_p_bound,\n",
    "                                                     lower_bound), upper_bound)\n",
    "\n",
    "            p_t = p - bounded_lr_t\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'final_lr': float(self.final_lr),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'gamma': float(self.gamma),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'weight_decay': self.weight_decay,\n",
    "                  'amsbound': self.amsbound}\n",
    "        base_config = super(AdaBound, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
